{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corona : 5250\n",
      "covid19 : 5428\n",
      "covid-19 : 11780\n",
      "16499\n",
      "22461\n"
     ]
    }
   ],
   "source": [
    "start_date=\"2020-05-06\"\n",
    "end_date=\"2020-05-12\"\n",
    "place=\"india\"\n",
    "no_tweets=20000\n",
    "\n",
    "#!pip install snscrape\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "#Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "keyword_list=[\"corona\",\"covid19\",'covid-19','wuhan','covid','pandemic','corona AND virus', 'coronavirus']\n",
    "limit=0\n",
    "for keyword in keyword_list:\n",
    "    #geocode:22.578005,88.358536,60km\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper('{} lang:en near:{} since:{} until:{}'.format(keyword,place,start_date,end_date)).get_items()):\n",
    "        if i>=no_tweets:\n",
    "            break\n",
    "        tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.hashtags, tweet.user.username, tweet.user.location, tweet.user.followersCount, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount , tweet.retweetedTweet, tweet.quotedTweet])\n",
    "    limit = limit+i\n",
    "    print(\"{} : {}\".format(keyword,i))\n",
    "    if limit>=no_tweets:\n",
    "        break\n",
    "\n",
    "#Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['datetime', 'tweet_id', 'text','hashtags', 'user_name', 'location','no_followers','no_replies','no_retweets','no_likes','no_quotes','retweet_id','quote_tweet_id'])\n",
    "\n",
    "temp = tweets_df2.drop_duplicates(subset='tweet_id')\n",
    "# Export dataframe into a CSV\n",
    "temp.to_csv('tweets_{}_{}_{}.csv'.format(start_date, end_date, place), sep=',', index=False)\n",
    "print(len(temp))\n",
    "print(len(tweets_df2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6047\n",
      "9913\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kolkata, 22.578005,88.358536,8.1km      #206.08 km2\n",
    "# chennai, 13.067439,80.237617,11.65km  #426 km2 \n",
    "# mumbai, 19.076090,72.877426,13.86km    #603.4 km2 \n",
    "#Delhi, 28.679079,77.069710,21.74km       #1,484 km2\n",
    "\n",
    "#lat, long taken from www.latlong.net and area from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6146"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_df2)\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date=\"12may2021\"\n",
    "# end_date=\"18may2021\"\n",
    "# city=\"kolkata\"\n",
    "\n",
    "# # !pip install snscrape\n",
    "# import snscrape.modules.twitter as sntwitter\n",
    "# import pandas as pd\n",
    "\n",
    "# # Creating list to append tweet data to\n",
    "# tweets_list2 = []\n",
    "# keyword_list=[\"corona\",\"covid19\",'wuhan','covid','pandemic','corona AND virus']\n",
    "# limit=0\n",
    "# for keyword in keyword_list:\n",
    "#     # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "#     for i,tweet in enumerate(sntwitter.TwitterSearchScraper(' {}.format(keyword) lang:en geocode:22.578005,88.358536,50km  since:2021-05-12 until:2021-05-18').get_items()):\n",
    "#         if i>9999:\n",
    "#             break\n",
    "#         tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.location])\n",
    "#     limit +=i\n",
    "#     print(\"{} : {}\".format(keyword,limit))\n",
    "#     if limit>9999:\n",
    "#         break\n",
    "# # Creating a dataframe from the tweets list above\n",
    "# tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username', 'location'])\n",
    "# print(len(tweets_df2))\n",
    "\n",
    "# # Export dataframe into a CSV\n",
    "# tweets_df2.to_csv('tweets_{}_{}_{}.csv'.format(start_date, end_date, city), sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e93b212ec87831c906be03bb3f13c1b669caa07610db076bd72cc3f448a7baba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
